{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUH0BSBb8RtG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'PO_Data.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic analysis of the dataset\n",
        "\n",
        "# Checking for missing values\n",
        "missing_values = data.isnull().sum()\n",
        "\n",
        "# Data types of the columns\n",
        "data_types = data.dtypes\n",
        "\n",
        "# Basic statistics\n",
        "basic_stats = data.describe(include='all', datetime_is_numeric=True)\n",
        "\n",
        "missing_values, data_types, basic_stats"
      ],
      "metadata": {
        "id": "5SlESQZJ8Xet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preparation for Demand Forecasting\n",
        "\n",
        "# Convert date fields to datetime\n",
        "data['PO_CREATED_AT'] = pd.to_datetime(data['PO_CREATED_AT'])\n",
        "data['PO_ARRIVED_AT'] = pd.to_datetime(data['PO_ARRIVED_AT'])\n",
        "\n",
        "# Select relevant features for demand forecasting\n",
        "forecasting_data = data[['PO_CREATED_AT', 'SKU', 'ORDERED_QTY']]\n",
        "\n",
        "# Aggregate data to monthly level\n",
        "forecasting_data['Month'] = forecasting_data['PO_CREATED_AT'].dt.to_period('M')\n",
        "monthly_demand = forecasting_data.groupby(['Month', 'SKU']).agg(Total_Ordered_Qty=('ORDERED_QTY', 'sum')).reset_index()\n",
        "\n",
        "# Checking the aggregated data\n",
        "monthly_demand.head()"
      ],
      "metadata": {
        "id": "5rXTUn0d8a7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Selecting a SKU with the most records\n",
        "top_sku = monthly_demand['SKU'].value_counts().idxmax()\n",
        "\n",
        "# Extracting time series for the selected SKU\n",
        "sku_demand_data = monthly_demand[monthly_demand['SKU'] == top_sku].set_index('Month')\n",
        "sku_demand_data = sku_demand_data.sort_index()\n",
        "\n",
        "# Plotting the time series for the selected SKU\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(sku_demand_data.index.to_timestamp(), sku_demand_data['Total_Ordered_Qty'], marker='o')\n",
        "plt.title(f'Monthly Demand for SKU: {top_sku}')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Total Ordered Quantity')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Preparing for ARIMA model\n",
        "# We'll take a subset of the data to train the model and use the rest for testing\n",
        "train_data = sku_demand_data['Total_Ordered_Qty'][:-12]  # Leaving last 12 months for testing\n",
        "test_data = sku_demand_data['Total_Ordered_Qty'][-12:]\n",
        "\n",
        "# Fit an ARIMA model\n",
        "# Note: The parameters (p,d,q) are set to (1,1,1) as a starting point. These can be optimized.\n",
        "model = ARIMA(train_data, order=(1, 1, 1))\n",
        "fitted_model = model.fit()\n",
        "\n",
        "# Forecast\n",
        "forecast = fitted_model.forecast(steps=12)\n",
        "\n",
        "# Plotting forecast against actual data\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(train_data.index.to_timestamp(), train_data, label='Train Data', marker='o')\n",
        "plt.plot(test_data.index.to_timestamp(), test_data, label='Test Data', marker='o')\n",
        "plt.plot(test_data.index.to_timestamp(), forecast, label='Forecast', marker='o')\n",
        "plt.title(f'ARIMA Forecast vs Actuals for SKU: {top_sku}')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Total Ordered Quantity')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9dsT-2j_8c7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.arima.model import ARIMAResults\n",
        "import itertools\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Define the p, d, and q parameters to take any value between 0 and 2\n",
        "p = d = q = range(0, 3)\n",
        "\n",
        "# Generate all different combinations of p, d, and q triplets\n",
        "pdq = list(itertools.product(p, d, q))\n",
        "\n",
        "# Grid search for the optimal ARIMA parameters\n",
        "best_score, best_cfg = float(\"inf\"), None\n",
        "\n",
        "for param in pdq:\n",
        "    try:\n",
        "        model = ARIMA(train_data, order=param)\n",
        "        results = model.fit()\n",
        "        forecast = results.forecast(steps=12)\n",
        "        mse = mean_squared_error(test_data, forecast)\n",
        "        if mse < best_score:\n",
        "            best_score, best_cfg = mse, param\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "# Best parameters\n",
        "best_cfg, best_score\n",
        "\n",
        "# Refitting the model with the best parameters and evaluating\n",
        "best_model = ARIMA(train_data, order=best_cfg)\n",
        "fitted_best_model = best_model.fit()\n",
        "\n",
        "# Forecast with the optimized model\n",
        "optimized_forecast = fitted_best_model.forecast(steps=12)\n",
        "\n",
        "# Evaluation metrics\n",
        "mae = mean_absolute_error(test_data, optimized_forecast)\n",
        "rmse = np.sqrt(mean_squared_error(test_data, optimized_forecast))\n",
        "\n",
        "optimized_forecast, mae, rmse"
      ],
      "metadata": {
        "id": "agssCUL-8hO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preparation for Vendor Performance Analysis\n",
        "\n",
        "# Convert date fields to datetime (if not already converted)\n",
        "data['PO_SHIPPED_AT'] = pd.to_datetime(data['PO_SHIPPED_AT'], errors='coerce')\n",
        "\n",
        "# Calculating delivery delay (in days)\n",
        "data['Delivery_Delay'] = (data['PO_ARRIVED_AT'] - data['PO_SHIPPED_AT']).dt.days\n",
        "\n",
        "# Calculating fulfillment accuracy (ratio of received qty to ordered qty)\n",
        "data['Fulfillment_Accuracy'] = data['TOTAL_RECEIVED_QTY'] / data['ORDERED_QTY']\n",
        "\n",
        "# Aggregate data by vendor\n",
        "vendor_performance = data.groupby('VENDOR_NAME').agg(\n",
        "    Average_Delay=('Delivery_Delay', 'mean'),\n",
        "    Average_Fulfillment_Accuracy=('Fulfillment_Accuracy', 'mean'),\n",
        "    Order_Frequency=('PO_NUMBER', 'nunique')\n",
        ").reset_index()\n",
        "\n",
        "# Checking the aggregated vendor performance data\n",
        "vendor_performance.head()"
      ],
      "metadata": {
        "id": "_9fAsyhD8nbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "\n",
        "# Removing records where 'Average_Delay' is NaN\n",
        "vendor_performance_cleaned = vendor_performance.dropna(subset=['Average_Delay'])\n",
        "\n",
        "# Preparing the data again for the regression model\n",
        "X_clean = vendor_performance_cleaned.drop(['VENDOR_NAME', 'Average_Delay'], axis=1)\n",
        "y_clean = vendor_performance_cleaned['Average_Delay']\n",
        "\n",
        "# Impute missing values in features\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "X_clean_imputed = imputer.fit_transform(X_clean)\n",
        "\n",
        "# Splitting the dataset into training and testing sets again\n",
        "X_train_clean, X_test_clean, y_train_clean, y_test_clean = train_test_split(X_clean_imputed, y_clean, test_size=0.2, random_state=42)\n",
        "\n",
        "# Linear Regression Model\n",
        "model_clean = LinearRegression()\n",
        "model_clean.fit(X_train_clean, y_train_clean)\n",
        "\n",
        "# Predictions and Model Evaluation\n",
        "y_pred_clean = model_clean.predict(X_test_clean)\n",
        "mse_clean = mean_squared_error(y_test_clean, y_pred_clean)\n",
        "r2_clean = r2_score(y_test_clean, y_pred_clean)\n",
        "\n",
        "mse_clean, r2_clean"
      ],
      "metadata": {
        "id": "y8zXENSn8qpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert PO_CLOSED_AT to datetime\n",
        "data['PO_CLOSED_AT'] = pd.to_datetime(data['PO_CLOSED_AT'], errors='coerce')\n",
        "\n",
        "# Calculating PO Cycle Time (in days)\n",
        "data['PO_Cycle_Time'] = (data['PO_CLOSED_AT'] - data['PO_CREATED_AT']).dt.days\n",
        "\n",
        "# Preparing data for the regression model\n",
        "# Selecting relevant features\n",
        "cycle_time_data = data[['WAREHOUSE_ID', 'VENDOR_NAME', 'ORDERED_QTY', 'PO_Cycle_Time']]\n",
        "\n",
        "# Handling missing values in PO_Cycle_Time\n",
        "# Dropping rows where PO_Cycle_Time is NaN\n",
        "cycle_time_data_cleaned = cycle_time_data.dropna(subset=['PO_Cycle_Time'])\n",
        "\n",
        "# Encoding the 'VENDOR_NAME' as it's a categorical variable\n",
        "cycle_time_data_cleaned = pd.get_dummies(cycle_time_data_cleaned, columns=['VENDOR_NAME'])\n",
        "\n",
        "# Defining the feature matrix (X) and the target variable (y)\n",
        "X_cycle_time = cycle_time_data_cleaned.drop('PO_Cycle_Time', axis=1)\n",
        "y_cycle_time = cycle_time_data_cleaned['PO_Cycle_Time']\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train_cycle, X_test_cycle, y_train_cycle, y_test_cycle = train_test_split(X_cycle_time, y_cycle_time, test_size=0.2, random_state=42)\n",
        "\n",
        "# Checking the first few rows of the feature matrix\n",
        "X_train_cycle.head()"
      ],
      "metadata": {
        "id": "TWaOaWcq8uX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear Regression Model for PO Cycle Time Prediction\n",
        "model_cycle_time = LinearRegression()\n",
        "model_cycle_time.fit(X_train_cycle, y_train_cycle)\n",
        "\n",
        "# Predictions and Model Evaluation\n",
        "y_pred_cycle_time = model_cycle_time.predict(X_test_cycle)\n",
        "mse_cycle_time = mean_squared_error(y_test_cycle, y_pred_cycle_time)\n",
        "r2_cycle_time = r2_score(y_test_cycle, y_pred_cycle_time)\n",
        "\n",
        "mse_cycle_time, r2_cycle_time"
      ],
      "metadata": {
        "id": "Uc7GCQzB8ywi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Random Forest Regressor for PO Cycle Time Prediction\n",
        "rf_model_cycle_time = RandomForestRegressor(random_state=42)\n",
        "rf_model_cycle_time.fit(X_train_cycle, y_train_cycle)\n",
        "\n",
        "# Predictions and Model Evaluation\n",
        "y_pred_rf_cycle_time = rf_model_cycle_time.predict(X_test_cycle)\n",
        "mse_rf_cycle_time = mean_squared_error(y_test_cycle, y_pred_rf_cycle_time)\n",
        "r2_rf_cycle_time = r2_score(y_test_cycle, y_pred_rf_cycle_time)\n",
        "\n",
        "mse_rf_cycle_time, r2_rf_cycle_time"
      ],
      "metadata": {
        "id": "jm174xUO81f4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Parameters for Grid Search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Grid Search with Cross-Validation\n",
        "rf_grid_search = GridSearchCV(estimator=RandomForestRegressor(random_state=42),\n",
        "                              param_grid=param_grid,\n",
        "                              cv=3,\n",
        "                              n_jobs=-1,\n",
        "                              verbose=2)\n",
        "\n",
        "rf_grid_search.fit(X_train_cycle, y_train_cycle)\n",
        "\n",
        "# Best parameters\n",
        "rf_best_params = rf_grid_search.best_params_\n",
        "rf_best_params"
      ],
      "metadata": {
        "id": "bKOi1rms84IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preparation for On-Time Delivery Prediction\n",
        "\n",
        "# Creating the target variable (1 if on-time, 0 if late)\n",
        "data['On_Time_Delivery'] = ((data['PO_ARRIVED_AT'] - data['PO_CREATED_AT']).dt.days <= 5).astype(int)\n",
        "\n",
        "# Selecting relevant features\n",
        "delivery_data = data[['WAREHOUSE_ID', 'VENDOR_NAME', 'ORDERED_QTY', 'On_Time_Delivery']]\n",
        "\n",
        "# Handling missing values and encoding categorical variables\n",
        "delivery_data_cleaned = delivery_data.dropna()\n",
        "delivery_data_cleaned = pd.get_dummies(delivery_data_cleaned, columns=['VENDOR_NAME'])\n",
        "\n",
        "# Defining the feature matrix (X) and the target variable (y)\n",
        "X_delivery = delivery_data_cleaned.drop('On_Time_Delivery', axis=1)\n",
        "y_delivery = delivery_data_cleaned['On_Time_Delivery']\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train_delivery, X_test_delivery, y_train_delivery, y_test_delivery = train_test_split(X_delivery, y_delivery, test_size=0.2, random_state=42)\n",
        "\n",
        "# Checking the balance of the target variable\n",
        "y_delivery.value_counts()"
      ],
      "metadata": {
        "id": "6eOSWQcx87Ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Training the Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "rf_classifier.fit(X_train_delivery, y_train_delivery)\n",
        "\n",
        "# Predictions on the test set\n",
        "y_pred_delivery = rf_classifier.predict(X_test_delivery)\n",
        "\n",
        "# Evaluation metrics\n",
        "accuracy = accuracy_score(y_test_delivery, y_pred_delivery)\n",
        "roc_auc = roc_auc_score(y_test_delivery, y_pred_delivery)\n",
        "classification_rep = classification_report(y_test_delivery, y_pred_delivery)\n",
        "\n",
        "accuracy, roc_auc, classification_rep"
      ],
      "metadata": {
        "id": "Xqghgno49Afu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Re-importing the dataset\n",
        "data = pd.read_csv(file_path, parse_dates=['PO_CREATED_AT', 'PO_ARRIVED_AT', 'PO_SHIPPED_AT', 'PO_CLOSED_AT'])\n",
        "\n",
        "# Calculating PO Cycle Time (in days)\n",
        "data['PO_Cycle_Time'] = (data['PO_CLOSED_AT'] - data['PO_CREATED_AT']).dt.days\n",
        "\n",
        "# Preparing data for the regression model\n",
        "cycle_time_data = data[['WAREHOUSE_ID', 'VENDOR_NAME', 'ORDERED_QTY', 'PO_Cycle_Time']]\n",
        "\n",
        "# Dropping rows with NaN values in PO_Cycle_Time and encoding categorical variables\n",
        "cycle_time_data_cleaned = cycle_time_data.dropna(subset=['PO_Cycle_Time'])\n",
        "cycle_time_data_cleaned = pd.get_dummies(cycle_time_data_cleaned, columns=['VENDOR_NAME'])\n",
        "\n",
        "# Defining the feature matrix (X) and the target variable (y)\n",
        "X_cycle_time = cycle_time_data_cleaned.drop('PO_Cycle_Time', axis=1)\n",
        "y_cycle_time = cycle_time_data_cleaned['PO_Cycle_Time']\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train_cycle, X_test_cycle, y_train_cycle, y_test_cycle = train_test_split(X_cycle_time, y_cycle_time, test_size=0.2, random_state=42)\n",
        "\n",
        "# Random Forest Regressor for PO Cycle Time Prediction\n",
        "rf_model_cycle_time = RandomForestRegressor(random_state=42)\n",
        "rf_model_cycle_time.fit(X_train_cycle, y_train_cycle)\n",
        "\n",
        "# Predictions and Model Evaluation\n",
        "y_pred_cycle_time = rf_model_cycle_time.predict(X_test_cycle)\n",
        "mse_cycle_time = mean_squared_error(y_test_cycle, y_pred_cycle_time)\n",
        "r2_cycle_time = r2_score(y_test_cycle, y_pred_cycle_time)\n",
        "\n",
        "mse_cycle_time, r2_cycle_time"
      ],
      "metadata": {
        "id": "l6K6d5c09Dg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-importing the dataset for on-time delivery prediction\n",
        "data = pd.read_csv(file_path, parse_dates=['PO_CREATED_AT', 'PO_ARRIVED_AT', 'PO_SHIPPED_AT', 'PO_CLOSED_AT'])\n",
        "\n",
        "# Creating the target variable for on-time delivery (1 if within 5 days, 0 otherwise)\n",
        "data['On_Time_Delivery'] = ((data['PO_ARRIVED_AT'] - data['PO_CREATED_AT']).dt.days <= 5).astype(int)\n",
        "\n",
        "# Selecting relevant features and handling missing values\n",
        "delivery_data = data[['WAREHOUSE_ID', 'VENDOR_NAME', 'ORDERED_QTY', 'On_Time_Delivery']]\n",
        "delivery_data_cleaned = delivery_data.dropna()\n",
        "delivery_data_cleaned = pd.get_dummies(delivery_data_cleaned, columns=['VENDOR_NAME'])\n",
        "\n",
        "# Defining the feature matrix (X) and the target variable (y)\n",
        "X_delivery = delivery_data_cleaned.drop('On_Time_Delivery', axis=1)\n",
        "y_delivery = delivery_data_cleaned['On_Time_Delivery']\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train_delivery, X_test_delivery, y_train_delivery, y_test_delivery = train_test_split(X_delivery, y_delivery, test_size=0.2, random_state=42)\n",
        "\n",
        "# Training the Random Forest Classifier for on-time delivery prediction\n",
        "rf_classifier_delivery = RandomForestClassifier(random_state=42)\n",
        "rf_classifier_delivery.fit(X_train_delivery, y_train_delivery)\n",
        "\n",
        "# Predictions and Model Evaluation\n",
        "y_pred_delivery = rf_classifier_delivery.predict(X_test_delivery)\n",
        "accuracy_delivery = accuracy_score(y_test_delivery, y_pred_delivery)\n",
        "roc_auc_delivery = roc_auc_score(y_test_delivery, y_pred_delivery)\n",
        "classification_rep_delivery = classification_report(y_test_delivery, y_pred_delivery)\n",
        "\n",
        "accuracy_delivery, roc_auc_delivery, classification_rep_delivery"
      ],
      "metadata": {
        "id": "NCnwyG2d9Hvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting date columns to datetime format\n",
        "date_columns = ['PO_CREATED_AT', 'PO_ARRIVED_AT', 'PO_SHIPPED_AT', 'PO_CLOSED_AT']\n",
        "data[date_columns] = data[date_columns].apply(pd.to_datetime, errors='coerce')\n",
        "\n",
        "# Checking the conversion\n",
        "data[date_columns].dtypes"
      ],
      "metadata": {
        "id": "iVUFjgyC9MfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling missing values\n",
        "\n",
        "# Filling missing values in 'NOTE', 'DESCRIPTION', 'MASTER_ID', and 'MPN' with 'Unknown'\n",
        "fill_columns = ['NOTE', 'DESCRIPTION', 'MASTER_ID', 'MPN']\n",
        "data[fill_columns] = data[fill_columns].fillna('Unknown')\n",
        "\n",
        "# For 'TOTAL_RECEIVED_QTY', filling missing values with median\n",
        "data['TOTAL_RECEIVED_QTY'] = data['TOTAL_RECEIVED_QTY'].fillna(data['TOTAL_RECEIVED_QTY'].median())\n",
        "\n",
        "# Assessing the impact of missing values in 'PO_SHIPPED_AT' and 'PO_CLOSED_AT'\n",
        "missing_shipped_closed = data[['PO_SHIPPED_AT', 'PO_CLOSED_AT']].isnull().mean()\n",
        "\n",
        "missing_shipped_closed, data[fill_columns].isnull().sum(), data['TOTAL_RECEIVED_QTY'].isnull().sum()"
      ],
      "metadata": {
        "id": "ikVeJcQs9QqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Creating a feature for on-time arrival\n",
        "data['ON_TIME_DELIVERY'] = np.where((data['PO_ARRIVED_AT'] - data['PO_CREATED_AT']).dt.days <= 5, 1, 0)\n",
        "\n",
        "# Dropping 'PO_SHIPPED_AT' and 'PO_CLOSED_AT' due to high missing values\n",
        "data = data.drop(columns=['PO_SHIPPED_AT', 'PO_CLOSED_AT'])\n",
        "\n",
        "# Checking the new feature and the updated dataset\n",
        "data[['PO_CREATED_AT', 'PO_ARRIVED_AT', 'ON_TIME_DELIVERY']].head(), data.info()"
      ],
      "metadata": {
        "id": "C7T4nCW89Tgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Exploratory Data Analysis\n",
        "\n",
        "# Summary statistics for numerical features\n",
        "numerical_summary = data.describe()\n",
        "\n",
        "# Distribution of the target variable (ON_TIME_DELIVERY)\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(data['ON_TIME_DELIVERY'])\n",
        "plt.title('Distribution of On-Time Delivery')\n",
        "plt.xlabel('On-Time Delivery')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# Correlation heatmap for numerical features\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(data.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()\n",
        "\n",
        "numerical_summary"
      ],
      "metadata": {
        "id": "HjJqViBS9V7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MaxAbsScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Selecting numerical and categorical features\n",
        "numerical_features = ['ORDERED_QTY', 'TOTAL_RECEIVED_QTY']\n",
        "categorical_features = ['VENDOR_NAME', 'WAREHOUSE_ID', 'SKU', 'MASTER_ID', 'MPN']\n",
        "\n",
        "# Preparing the features (X) and target (y)\n",
        "X = data[numerical_features + categorical_features]\n",
        "y = data['ON_TIME_DELIVERY']\n",
        "\n",
        "# Creating a column transformer for preprocessing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', MaxAbsScaler(), numerical_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fitting the preprocessor\n",
        "X_train = preprocessor.fit_transform(X_train)\n",
        "X_test = preprocessor.transform(X_test)\n",
        "\n",
        "X_train.shape, X_test.shape"
      ],
      "metadata": {
        "id": "ACNzo5Pa9Y4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Applying PCA for dimensionality reduction\n",
        "# We choose a number of components that explains a substantial amount of variance\n",
        "pca = PCA(n_components=0.95)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "# Checking the shape after PCA\n",
        "X_train_pca.shape, X_test_pca.shape, pca.n_components_"
      ],
      "metadata": {
        "id": "vyomBqao9dCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# Reducing the number of components in Truncated SVD to address memory issue\n",
        "svd = TruncatedSVD(n_components=500)\n",
        "try:\n",
        "    X_train_svd = svd.fit_transform(X_train)\n",
        "    X_test_svd = svd.transform(X_test)\n",
        "    svd_success = True\n",
        "except Exception as e:\n",
        "    svd_success = False\n",
        "    svd_error = str(e)\n",
        "\n",
        "svd_success, svd_error if not svd_success else (X_train_svd.shape, X_test_svd.shape, svd.explained_variance_ratio_.sum())"
      ],
      "metadata": {
        "id": "BdKPoyU19ihE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Logistic Regression\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train_svd, y_train)\n",
        "log_reg_pred = log_reg.predict(X_test_svd)\n",
        "print(\"Logistic Regression Classification Report:\")\n",
        "print(classification_report(y_test, log_reg_pred))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, log_reg_pred))\n",
        "\n",
        "# XGBoost\n",
        "xgb = XGBClassifier()\n",
        "xgb.fit(X_train_svd, y_train)\n",
        "xgb_pred = xgb.predict(X_test_svd)\n",
        "print(\"XGBoost Classification Report:\")\n",
        "print(classification_report(y_test, xgb_pred))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, xgb_pred))\n",
        "\n",
        "# Feature Importance from XGBoost (for the original feature set, before SVD)\n",
        "xgb_feature_importance = xgb.feature_importances_"
      ],
      "metadata": {
        "id": "XvHiaC909pFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert date columns to datetime format\n",
        "data['PO_CREATED_AT'] = pd.to_datetime(data['PO_CREATED_AT'])\n",
        "data['PO_ARRIVED_AT'] = pd.to_datetime(data['PO_ARRIVED_AT'])\n",
        "\n",
        "# Create a new feature for on-time delivery (1 if on time, 0 if late)\n",
        "data['ON_TIME_DELIVERY'] = (data['PO_ARRIVED_AT'] - data['PO_CREATED_AT']).dt.days <= 5\n",
        "data['ON_TIME_DELIVERY'] = data['ON_TIME_DELIVERY'].astype(int)\n",
        "\n",
        "# Selecting features and target\n",
        "features = ['VENDOR_NAME', 'WAREHOUSE_ID', 'SKU', 'MASTER_ID', 'MPN', 'ORDERED_QTY', 'TOTAL_RECEIVED_QTY']\n",
        "X = data[features]\n",
        "y = data['ON_TIME_DELIVERY']\n",
        "\n",
        "# Preprocessing: Encoding categorical variables and scaling numerical variables\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), ['ORDERED_QTY', 'TOTAL_RECEIVED_QTY']),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['VENDOR_NAME', 'WAREHOUSE_ID', 'SKU', 'MASTER_ID', 'MPN'])\n",
        "    ])\n",
        "\n",
        "# Splitting the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply preprocessing\n",
        "X_train = preprocessor.fit_transform(X_train)\n",
        "X_test = preprocessor.transform(X_test)\n",
        "\n",
        "# Dimensionality reduction using Truncated SVD\n",
        "svd = TruncatedSVD(n_components=50)  # Adjust the number of components based on your system\n",
        "X_train_svd = svd.fit_transform(X_train)\n",
        "X_test_svd = svd.transform(X_test)\n",
        "\n",
        "# Logistic Regression Model\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train_svd, y_train)\n",
        "log_reg_pred = log_reg.predict(X_test_svd)\n",
        "print(\"Logistic Regression Classification Report:\")\n",
        "print(classification_report(y_test, log_reg_pred))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, log_reg_pred))\n",
        "\n",
        "# XGBoost Model\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "xgb.fit(X_train_svd, y_train)\n",
        "xgb_pred = xgb.predict(X_test_svd)\n",
        "print(\"XGBoost Classification Report:\")\n",
        "print(classification_report(y_test, xgb_pred))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, xgb_pred))\n",
        "\n",
        "# Feature importance from XGBoost (post-SVD feature space)\n",
        "xgb_feature_importance = xgb.feature_importances_"
      ],
      "metadata": {
        "id": "xUJ5a6xo9uy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression, ARDRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from xgboost import XGBClassifier\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "\n",
        "\n",
        "# Data preprocessing steps\n",
        "# ...\n",
        "\n",
        "# Apply preprocessing, Truncated SVD, and split the dataset\n",
        "# ...\n",
        "\n",
        "# Logistic Regression Model\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train_svd, y_train)\n",
        "log_reg_pred = log_reg.predict(X_test_svd)\n",
        "\n",
        "# XGBoost Model\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "xgb.fit(X_train_svd, y_train)\n",
        "xgb_pred = xgb.predict(X_test_svd)\n",
        "\n",
        "# ARD Regression Model\n",
        "ard = ARDRegression()\n",
        "ard.fit(X_train_svd, y_train)\n",
        "ard_pred = ard.predict(X_test_svd)\n",
        "# Convert predictions to binary outcomes\n",
        "ard_pred = [1 if p > 0.5 else 0 for p in ard_pred]\n",
        "\n",
        "# Evaluation\n",
        "print(\"Logistic Regression Classification Report:\")\n",
        "print(classification_report(y_test, log_reg_pred))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, log_reg_pred))\n",
        "\n",
        "print(\"XGBoost Classification Report:\")\n",
        "print(classification_report(y_test, xgb_pred))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, xgb_pred))\n",
        "\n",
        "print(\"ARD Regression Classification Report:\")\n",
        "print(classification_report(y_test, ard_pred))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, ard_pred))\n",
        "\n",
        "# Feature Importance Analysis\n",
        "# XGBoost feature importance\n",
        "xgb_feature_importance = xgb.feature_importances_\n",
        "print(\"XGBoost Feature Importance:\")\n",
        "print(xgb_feature_importance)\n",
        "\n",
        "# ARDRegression coefficients\n",
        "ard_coefficients = ard.coef_\n",
        "print(\"ARDRegression Coefficients (indicative of feature relevance):\")\n",
        "print(ard_coefficients)\n",
        "\n",
        "# Note: Interpretation of these values requires mapping back to original feature space if needed"
      ],
      "metadata": {
        "id": "4xQorYFO9vo6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}